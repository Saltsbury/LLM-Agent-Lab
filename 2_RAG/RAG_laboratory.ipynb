{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 课程基本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "- **课程名称**：大数据与商务智能 - 检索增强生成（RAG）系统实践\n",
    "- **授课对象**：中山大学工商管理非全专硕MBA学生\n",
    "- **课时**：3学时（上机课）\n",
    "- **前置知识**：基础Python编程能力，LLM基础应用能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "完成本课程后，您将能够：\n",
    "1. 独立搭建基于Milvus和LlamaIndex的RAG系统，实现企业知识库检索\n",
    "2. 掌握向量数据库的索引优化和性能调优方法，提升检索效率30%以上\n",
    "3. 设计并实现混合检索（密集+稀疏向量）解决方案，提高查询准确率\n",
    "4. 运用Cherry Studio可视化工具构建完整的企业知识库应用\n",
    "5. 具备RAG系统部署和维护的基本能力，识别并解决常见问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2.1 Chroma部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "- 纯Python安装（`pip install`即可），无需Docker/部署，1分钟装完\n",
    "- 能清晰展示「文档向量化→向量入库→相似检索→LLM生成」全流程\n",
    "- 完全本地运行，无需联网（适合课堂演示）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.1.1 Chroma 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "可能用时较长，请耐心等待"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb python-dotenv langchain langchain-community sentence-transformers openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.1.2 Chroma 相关LLM组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "首先检查Ollama中的本地模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "我们需要使用嵌入模型和生成模型，分别在2.1.2.1和2.1.2.2节中进行配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### 2.1.2.1 配置Ollama嵌入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "使用本地知识库需要嵌入模型`bge-m3`，请执行下方命令（用时较短）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull bge-m3:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "下面验证嵌入模型是否可用：若下方代码运行后输出前5个向量值，说明模型可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证bge-m3模型是否可用\n",
    "import requests\n",
    "# 调用Ollama的embeddings接口，测试bge-m3向量化\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/embeddings\",\n",
    "    json={\"model\": \"bge-m3:latest\", \"prompt\": \"MBA财务管理\"}\n",
    ")\n",
    "print(response.json()[\"embedding\"][:5])  # 若输出前5个向量值，说明模型可用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 配置Ollama生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "为了体现离线部署特性，我们建议使用Ollama中的本地模型，如千问`qwen3`模型，亦可跳过此步，更换为自己喜欢的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "运行下方代码块即可安装qwen3模型（可能用时较长，请耐心等待，约15分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull qwen3-vl:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 2.1.3 体验Chroma RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### 2.1.3.1 RAG处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import ollama\n",
    "\n",
    "class LocalRAG:\n",
    "    def __init__(self, OLLAMA_API_URL=\"http://localhost:11434/api\", EMBED_MODEL=\"bge-m3:latest\", LLM_MODEL=\"qwen3-vl:8b\", business_cases=None):\n",
    "        \"\"\"\n",
    "        初始化一个完整的本地RAG系统\n",
    "        params:\n",
    "            OLLAMA_API_URL: Ollama本地API地址，默认http://localhost:11434/api\n",
    "            EMBED_MODEL: 本地嵌入模型名称，默认使用bge-m3:latest\n",
    "            LLM_MODEL: 本地大模型名称，默认使用千问Qwen3-VL:8B\n",
    "            business_cases: 可选，自定义MBA商业案例列表\n",
    "        \"\"\"\n",
    "        self.OLLAMA_API_URL = OLLAMA_API_URL            # Ollama本地API地址\n",
    "        self.EMBED_MODEL = EMBED_MODEL                  # 本地嵌入模型，和ollama list中一致\n",
    "        self.LLM_MODEL = LLM_MODEL                      # 本地大模型名称，和ollama list中一致\n",
    "\n",
    "        # 初始化Chroma（内存模式）\n",
    "        print(\"正在初始化Chroma向量库\")\n",
    "        self.chroma_client = chromadb.Client(Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "        self.chroma_client.reset()  # 清空旧数据\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=\"mba_business_cases\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        print(\"Chroma向量库初始化成功\")\n",
    "\n",
    "        # 加载商业案例到Chroma向量库\n",
    "        print(\"正在加载MBA商业案例到Chroma向量库\")\n",
    "        self.load_mba_cases(business_cases)\n",
    "        print(\"MBA商业案例加载完成\")\n",
    "\n",
    "        print(\"本地RAG系统初始化完成\")\n",
    "\n",
    "    def load_mba_cases(self, business_cases=None):\n",
    "        \"\"\"\n",
    "        加载MBA商业案例到Chroma向量库\n",
    "        params:\n",
    "            business_cases: 可选，自定义MBA商业案例列表\n",
    "        \"\"\"\n",
    "        # 加载MBA商业案例\n",
    "        business_cases = [\n",
    "            {\"id\": \"1\", \"text\": \"案例1：特斯拉的定价策略——采用撇脂定价法，初期以高价锁定高端用户收回研发成本，后期通过规模化生产降低成本，逐步降价渗透大众市场，最终实现销量和利润双增长；核心逻辑是“高价卡位+低价放量”的组合策略。\"},\n",
    "            {\"id\": \"2\", \"text\": \"案例2：星巴克供应链管理——全球统一采购阿拉比卡咖啡豆，在产地附近建立烘焙厂降低物流成本，通过“门店库存动态调整系统”优化补货效率，将缺货率控制在1%以内；核心是“集中采购+本地化加工+数据驱动库存”。\"},\n",
    "            {\"id\": \"3\", \"text\": \"案例3：阿里新零售战略——整合淘宝/天猫线上流量和盒马鲜生线下门店，通过“用户行为数据中台”分析消费习惯，实现“线上下单、30分钟送达”的即时零售模式；核心是“线上线下数据打通+履约效率提升”。\"},\n",
    "            {\"id\": \"4\", \"text\": \"案例4：华为研发投入——连续10年将营收的15%以上投入研发，聚焦5G通信、芯片设计等核心技术，累计专利数超8万件，构建了难以复制的技术壁垒；核心是“长期主义+核心技术自主可控”。\"}\n",
    "        ] if not business_cases else business_cases\n",
    "\n",
    "        # 向量化并入库\n",
    "        texts = [case[\"text\"] for case in business_cases]\n",
    "        ids = [case[\"id\"] for case in business_cases]\n",
    "        embeddings = [self.get_embedding(text) for text in texts]\n",
    "        self.collection.add(documents=texts, embeddings=embeddings, ids=ids)\n",
    "        print(\"案例已通过bge-m3向量化，存入Chroma\")\n",
    "\n",
    "    def load_documents(self, nodes):\n",
    "        \"\"\"\n",
    "        Load processed documents (nodes) into Chroma vector database\n",
    "        params:\n",
    "            nodes: List of Document nodes from load_and_process_documents\n",
    "        \"\"\"\n",
    "        texts = [node.text for node in nodes]\n",
    "        ids = [str(i) for i in range(len(nodes))]\n",
    "        embeddings = [self.get_embedding(text) for text in texts]\n",
    "        self.collection.add(documents=texts, embeddings=embeddings, ids=ids)\n",
    "        print(f\"Loaded {len(nodes)} document chunks into Chroma\")\n",
    "\n",
    "    # 调用本地嵌入模型bge-m3生成向量\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"调用本地bge-m3生成文本向量\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.OLLAMA_API_URL}/embeddings\",\n",
    "                json={\"model\": self.EMBED_MODEL, \"prompt\": text},\n",
    "                timeout=15\n",
    "            )\n",
    "            return response.json()[\"embedding\"]\n",
    "        except Exception as e:\n",
    "            print(f\"bge-m3调用失败：{e}\")\n",
    "            raise e\n",
    "\n",
    "    # 调用本地生成模型Qwen3-VL:8B生成回答\n",
    "    def call_local_llm(self, prompt, context):\n",
    "        \"\"\"\n",
    "        调用本地Qwen3-VL:8B生成回答\n",
    "        :param prompt: 学生的原始提问\n",
    "        :param context: Chroma检索到的相关MBA案例（上下文信息）\n",
    "        \"\"\"\n",
    "        # 构建RAG提示词\n",
    "        rag_prompt = f\"\"\"\n",
    "        你是MBA商业课程的专业讲师，请基于以下参考资料回答问题，要求：\n",
    "        1. 仅使用参考资料中的信息，不编造内容；\n",
    "        2. 回答简洁、专业，贴合MBA课程的商业视角；\n",
    "        3. 重点突出案例中的核心策略和商业逻辑。\n",
    "\n",
    "        参考资料：\n",
    "        {context}\n",
    "\n",
    "        学生提问：\n",
    "        {prompt}\n",
    "        \"\"\"\n",
    "\n",
    "        # 使用Ollama Python库生成回答\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "            model=self.LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
    "            options={\n",
    "                \"temperature\": 0.6, \n",
    "                \"num_ctx\": 8192,        # num_ctx=上下文窗口大小\n",
    "                \"stream\": True          # 启用流式输出\n",
    "                } \n",
    "            )\n",
    "\n",
    "            # 提取回答内容\n",
    "            answer = response[\"message\"][\"content\"]\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            print(f\"生成模型调用失败：{e}\")\n",
    "            raise e\n",
    "\n",
    "    def query(self, question, top_k=2):\n",
    "        \"\"\"\n",
    "        RAG问答接口\n",
    "        params:\n",
    "            question: 学生提问\n",
    "            top_k: 检索相关案例数量\n",
    "        returns:\n",
    "            answer: 基于检索上下文的回答\n",
    "        \"\"\"\n",
    "        # 向量化学生提问\n",
    "        question_embedding = self.get_embedding(question)\n",
    "\n",
    "        # 从Chroma检索相关案例\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[question_embedding],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        retrieved_texts = results['documents'][0]  # 取出文本列表\n",
    "\n",
    "        print(f\"Chroma检索到最相似案例（相似度：{1 - results['distances'][0][0]:.2f}）：\\n{retrieved_texts[0]}\\n\")\n",
    "\n",
    "        # 构建上下文信息\n",
    "        context = \"\\n\".join(retrieved_texts)\n",
    "\n",
    "        # 调用本地LLM生成回答\n",
    "        print(\"正在调用本地生成LLM模型回答...\")\n",
    "        answer = self.call_local_llm(question, context)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 实例化一个本地RAG系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义商业案例\n",
    "rag_business_cases = [\n",
    "    {\"id\": \"1\", \"text\": \"案例1：特斯拉的定价策略——采用撇脂定价法，初期以高价锁定高端用户收回研发成本，后期通过规模化生产降低成本，逐步降价渗透大众市场，最终实现销量和利润双增长；核心逻辑是“高价卡位+低价放量”的组合策略。\"},\n",
    "    {\"id\": \"2\", \"text\": \"案例2：星巴克供应链管理——全球统一采购阿拉比卡咖啡豆，在产地附近建立烘焙厂降低物流成本，通过“门店库存动态调整系统”优化补货效率，将缺货率控制在1%以内；核心是“集中采购+本地化加工+数据驱动库存”。\"},\n",
    "    {\"id\": \"3\", \"text\": \"案例3：阿里新零售战略——整合淘宝/天猫线上流量和盒马鲜生线下门店，通过“用户行为数据中台”分析消费习惯，实现“线上下单、30分钟送达”的即时零售模式；核心是“线上线下数据打通+履约效率提升”。\"},\n",
    "    {\"id\": \"4\", \"text\": \"案例4：华为研发投入——连续10年将营收的15%以上投入研发，聚焦5G通信、芯片设计等核心技术，累计专利数超8万件，构建了难以复制的技术壁垒；核心是“长期主义+核心技术自主可控”。\"}\n",
    "]\n",
    "\n",
    "# 初始化本地RAG系统\n",
    "local_rag = LocalRAG(\n",
    "    OLLAMA_API_URL=\"http://localhost:11434/api\", \n",
    "    EMBED_MODEL=\"bge-m3:latest\", \n",
    "    LLM_MODEL=\"qwen3-vl:8b\",\n",
    "    # LLM_MODEL=\"deepseek-r1:7b\",\n",
    "    business_cases=rag_business_cases\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### 2.1.3.3 体验RAG问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "以下为本地运行的RAG+LLM回答，输出时间可能长达5分钟，期间卡顿是正常情况，请耐心等待回答完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提问\n",
    "query = \"哪些企业通过定价策略实现了市场渗透？请分析其核心逻辑。\"\n",
    "print(f\"提问：{query}\\n\")\n",
    "answer = local_rag.query(query, top_k=2)\n",
    "print(f\"RAG回答：\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 2.2 RAG优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 2.2.1 RAG 提示词工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### 2.2.2.1 RAG提示词基本格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "\n",
    "```markdown\n",
    "# 系统角色\n",
    "你是企业知识库问答专家，使用提供的检索结果回答问题。\n",
    "\n",
    "## 检索结果使用规则\n",
    "- 仅使用检索到的内容回答问题\n",
    "- 明确引用来源文档和页码\n",
    "- 对冲突信息标注\"信息冲突：来源A认为...来源B认为...\"\n",
    "\n",
    "## 输出格式\n",
    "**回答**：[基于检索内容的回答]\n",
    "**来源**：[引用文档列表]\n",
    "**检索建议**：[如果信息不足，建议补充的检索关键词]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### 2.2.1.2 RAG提示词优化技巧\n",
    "1. **指定回答风格**：\n",
    "    ```markdown\n",
    "    # 回答风格\n",
    "    - 使用简洁的要点形式回答\n",
    "    - 每点不超过20个字\n",
    "    - 重点内容加粗显示\n",
    "    ```\n",
    "\n",
    "2. **多轮检索提示**：\n",
    "    ```markdown\n",
    "    # 多轮检索优化\n",
    "    如果第一次检索结果不足以回答问题：\n",
    "    1. 分析缺失的信息\n",
    "    2. 生成补充检索关键词\n",
    "    3. 进行二次检索\n",
    "    4. 综合两次结果回答\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 2.2.2 文档加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_process_documents(\n",
    "    data_dir: str = \"../data\",\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 20\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    加载文档并进行分块处理\n",
    "    \"\"\"\n",
    "    # 1. 加载文档\n",
    "    print(f\"从 {data_dir} 加载文档...\")\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=data_dir,\n",
    "        recursive=True,\n",
    "        required_exts=[\".pdf\", \".docx\", \".txt\"],\n",
    "    )\n",
    "    \n",
    "    documents = reader.load_data()\n",
    "    print(f\"成功加载 {len(documents)} 个文档\")\n",
    "    \n",
    "    # 2. 添加元数据（扩展LLM课程的元数据管理）\n",
    "    for doc in documents:\n",
    "        file_name = doc.metadata.get(\"file_name\", \"unknown_source\")\n",
    "        doc.metadata[\"source\"] = file_name\n",
    "        doc.metadata[\"processed_date\"] = str(datetime.now().date())\n",
    "        doc.metadata[\"course\"] = \"RAG\"  # 新增课程标识\n",
    "        \n",
    "    # 3. 文档分块（优化LLM课程的分块策略）\n",
    "    print(f\"对文档进行分块处理 (chunk_size={chunk_size}, chunk_overlap={chunk_overlap})\")\n",
    "    parser = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separator=\"\\n\\n\"  # 优先按段落分割\n",
    "    )\n",
    "    \n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "    \n",
    "    print(f\"文档分块完成，共生成 {len(nodes)} 个文档块\")\n",
    "    print(f\"示例块内容: {nodes[0].text[:100]}...\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nodes = load_and_process_documents()\n",
    "    \n",
    "    # 保存处理结果\n",
    "    import pickle\n",
    "    with open(\"processed_nodes.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nodes, f)\n",
    "    print(\"文档处理结果已保存至 processed_nodes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have run the load_and_process_documents function\n",
    "nodes = load_and_process_documents(data_dir=\"../data\")  # 指向文档的路径\n",
    "\n",
    "# Initialize LocalRAG without business_cases\n",
    "documents_local_rag = LocalRAG(business_cases=None)\n",
    "# Load the processed documents\n",
    "documents_local_rag.load_documents(nodes)\n",
    "\n",
    "# Now you can query as usual\n",
    "query = \"哪些企业通过定价策略实现了市场渗透？请分析其核心逻辑。\"\n",
    "answer = documents_local_rag.query(query, top_k=2)\n",
    "print(f\"RAG回答：{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### 2.2.3 高级性能优化（仅作了解）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### 2.2.3.1 异步查询"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "```python\n",
    "async def batch_query(self, queries: list) -> list:\n",
    "    \"\"\"批量执行异步查询（RAG性能优化技术）\"\"\"\n",
    "    tasks = [self.async_query(q) for q in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "#### 2.2.3.2 索引优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "```python\n",
    "index_params_list = [\n",
    "    {\"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}},  # 基础索引\n",
    "    {\"index_type\": \"HNSW\", \"params\": {\"M\": 16, \"efConstruction\": 256}}  # 优化索引\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 2.3 使用Cherry Studio构建知识库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 2.3.1 Cherry Studio知识库功能概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Cherry Studio提供了完整的知识库管理功能，支持从文档导入、向量化处理到检索优化的全流程可视化操作。根据[官方文档](https://docs.cherry-ai.com/knowledge-base/knowledge-base)，其核心功能包括：\n",
    "\n",
    "- **多源文档导入**：支持PDF、Word、TXT等多种格式\n",
    "- **智能分块处理**：自动优化文档分块策略\n",
    "- **向量存储管理**：支持Milvus/FAISS等多种向量数据库\n",
    "- **检索策略配置**：灵活调整检索参数和相似度阈值\n",
    "- **知识库测试**：内置查询测试和结果分析工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "拓展资源：\n",
    "- **Cherry Studio官方文档**：https://docs.cherry-ai.com/knowledge-base/knowledge-base\n",
    "- **知识库最佳实践**：https://docs.cherry-ai.com/knowledge-base/best-practices\n",
    "- **高级检索策略**：https://docs.cherry-ai.com/knowledge-base/advanced-retrieval\n",
    "- **性能优化指南**：https://docs.cherry-ai.com/knowledge-base/performance-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 创建新知识库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "1. 启动Cherry Studio，进入\"知识库\"模块\n",
    "2. 点击\"新建知识库\"，填写基本信息：\n",
    "   - 名称：企业知识库\n",
    "   - 描述：用于存储和检索企业内部文档\n",
    "   - 存储位置：本地\n",
    "   - 向量数据库：Milvus（已在环境准备中配置）\n",
    "3. 点击\"创建\"，系统将初始化知识库结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 配置知识库参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "1. 进入知识库详情页，切换到\"设置\"标签\n",
    "2. 配置文档处理参数：\n",
    "   ```json\n",
    "   {\n",
    "     \"chunk_size\": 512,\n",
    "     \"chunk_overlap\": 50,\n",
    "     \"separator\": \"\\n\\n\",\n",
    "     \"include_metadata\": true\n",
    "   }\n",
    "   ```\n",
    "3. 配置嵌入模型：\n",
    "   - 模型名称：BAAI/bge-base-en-v1.5\n",
    "   - 批量大小：16\n",
    "   - 设备：自动\n",
    "4. 点击\"保存配置\"并应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### 2.3.2 文档导入与管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### 2.3.2.1 导入文档\n",
    "1. 进入\"文档管理\"标签，点击\"导入文档\"\n",
    "2. 选择导入方式：\n",
    "   - 方式1：本地上传（支持多文件批量上传）\n",
    "   - 方式2：目录同步（监控指定目录自动导入）\n",
    "3. 选择测试文档（可使用课程提供的sample_docs.zip）\n",
    "4. 点击\"开始导入\"，系统将自动处理文档\n",
    "\n",
    "#### 2.3.2.2 文档处理监控\n",
    "1. 导入过程中，可在\"任务中心\"查看处理进度\n",
    "2. 处理完成后，查看文档统计信息：\n",
    "   - 总文档数：X个\n",
    "   - 总文档块数：Y个\n",
    "   - 平均分块大小：Z tokens\n",
    "3. 检查是否有处理失败的文档，点击\"重试\"处理异常文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### 2.3.3 检索策略配置\n",
    "\n",
    "#### 2.3.3.1 基础检索配置\n",
    "1. 进入\"检索设置\"标签，配置基础参数：\n",
    "   - 相似度阈值：0.75\n",
    "   - 返回结果数：5\n",
    "   - 重排序：启用（基于BM25）\n",
    "2. 点击\"测试检索\"，输入测试查询：\"企业核心产品有哪些？\"\n",
    "3. 查看检索结果和相关性评分，调整参数优化结果\n",
    "\n",
    "#### 2.3.3.2 高级检索策略（混合检索配置）\n",
    "1. 在\"检索设置\"中启用\"混合检索\"\n",
    "2. 配置参数：\n",
    "   - 密集向量权重：0.7\n",
    "   - 稀疏向量权重：0.3\n",
    "   - 交叉编码器重排序：启用\n",
    "3. 点击\"保存并应用\"，系统将自动重建索引\n",
    "4. 对比启用前后的检索效果差异"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 2.3.4 知识库应用开发\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### 2.3.4.1 创建检索应用\n",
    "1. 进入\"应用构建\"模块，点击\"新建应用\"\n",
    "2. 选择模板：\"知识库问答应用\"\n",
    "3. 配置应用参数：\n",
    "   - 名称：企业知识库问答\n",
    "   - 关联知识库：选择之前创建的\"企业知识库\"\n",
    "   - 模型：deepseek-r1:7b等本地Ollama生成模型\n",
    "   - 提示词模板：使用RAG专用模板\n",
    "4. 点击\"创建应用\"，系统自动生成应用界面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### 2.3.4.2 应用测试与优化\n",
    "1. 进入应用详情页，点击\"预览\"\n",
    "2. 在测试界面输入问题，测试不同类型查询：\n",
    "   - 事实型：\"企业成立时间？\"\n",
    "   - 概念型：\"什么是核心竞争力？\"\n",
    "   - 分析型：\"分析产品市场优势\"\n",
    "3. 查看回答质量和引用来源，记录需要优化的案例\n",
    "4. 进入\"优化中心\"，针对低质量回答调整：\n",
    "   - 修改分块参数\n",
    "   - 优化提示词模板\n",
    "   - 调整检索策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 2.3.5 知识库自动更新\n",
    "\n",
    "1. 进入知识库\"设置\"→\"自动更新\"\n",
    "2. 配置更新策略：\n",
    "   - 更新频率：每周日凌晨2点\n",
    "   - 更新范围：新增文档和修改过的文档\n",
    "   - 通知方式：邮件通知\n",
    "3. 点击\"启用自动更新\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### 2.3.6 常见问题与解决方案\n",
    "\n",
    "#### 文档导入失败\n",
    "**问题**：PDF文档导入后内容为空  \n",
    "**解决方案**：\n",
    "1. 检查PDF是否加密或扫描件（需OCR处理）\n",
    "2. 进入\"设置\"→\"文档处理\"，启用\"高级PDF解析\"\n",
    "3. 重新导入文档\n",
    "\n",
    "#### 检索结果相关性低\n",
    "**问题**：查询结果与问题相关性差  \n",
    "**解决方案**：\n",
    "1. 降低相似度阈值（如从0.8调整至0.7）\n",
    "2. 增加chunk_overlap至100\n",
    "3. 尝试不同的嵌入模型（如切换至bge-large模型）\n",
    "\n",
    "#### 知识库体积过大\n",
    "**问题**：知识库文档过多导致检索缓慢  \n",
    "**解决方案**：\n",
    "1. 启用\"分层检索\"（设置知识库层级结构）\n",
    "2. 配置\"自动归档\"策略，将旧文档移至归档库\n",
    "3. 优化索引参数，使用IVF_FLAT索引类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## 2.4 课程作业：小型知识库系统开发\n",
    "\n",
    "### 2.4.1 作业目标\n",
    "使用Cherry Studio构建一个小型的知识库系统，实现文档管理、智能检索和问答功能。\n",
    "\n",
    "### 2.4.2 具体要求\n",
    "1. **知识库构建**：\n",
    "   - 导入至少10篇不同类型的文档\n",
    "   - 配置混合检索策略\n",
    "   - 优化分块和嵌入参数\n",
    "\n",
    "2. **应用开发**：\n",
    "   - 创建知识库问答应用\n",
    "   - 实现自定义提示词模板\n",
    "   - 添加结果可视化功能\n",
    "\n",
    "3. **性能优化**：\n",
    "   - 对比不同检索策略的效果\n",
    "   - 分析并优化低相关性查询案例\n",
    "   - 生成性能测试报告\n",
    "\n",
    "### 提交内容\n",
    "- 知识库配置截图\n",
    "- 应用界面截图\n",
    "- 5组测试查询的问答记录\n",
    "- 优化前后的性能对比报告\n",
    "- 技术总结（500字以内）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Agent-Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
