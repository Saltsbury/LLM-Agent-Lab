# 大语言模型（LLM）上机课讲义

## 课程基本信息
- **课程名称**：大数据与商务智能 - 大语言模型应用开发实践
- **授课对象**：中山大学工商管理非全专硕MBA学生
- **课时**：3学时（上机课）
- **前置知识**：基础Python编程

## 学习目标
完成本课程后，您将能够：
1. 独立搭建本地大语言模型运行环境
2. 使用Python脚本调用在线和本地LLM服务
3. 通过Cherry Studio可视化工具开发LLM应用
4. 设计结构化系统提示词提升AI响应质量
5. 开发简单的LLM应用解决实际业务问题

## 1. LLM服务准备

### 1.1.1 在线LLM API获取

#### 1.1.1.1 获取在线API

1. Deepseek（深度求索）
    
    Deepseek提供多语言大模型服务，适合代码生成和专业领域问答场景。官方文档：https://api-docs.deepseek.com/zh-cn/
    
    1. **注册Deepseek账号**  
        访问Deepseek官网：https://platform.deepseek.com
    
    2. **开通API服务**  
        - 登录后进入"控制台"→"API服务"
        - 点击"开通服务"，阅读并同意服务协议
    
    3. **创建API密钥**  
        - 在控制台左侧菜单选择"API密钥"
        - 点击"新建密钥"，输入密钥名称（如"llm-course"）
        - 保存生成的API Key（注意：仅显示一次，需及时复制）
    
    4. **获取API端点**  
        Deepseek通用对话API地址：`https://api.deepseek.com/v1/chat/completions`

2. 通义千问（阿里云）

    通义千问是阿里云提供的大语言模型服务，通过阿里云百炼平台提供API调用方式，适合中文场景应用开发。

    1. **注册阿里云账号**  
    访问阿里云官网：https://www.aliyun.com/

    2. **开通通义千问服务**  
        - 访问阿里云百炼平台：https://bailian.aliyun.com/
        - 点击"立即开通"，完成服务开通流程

    3. **创建API密钥**  
        - 登录后点击右上角"创建我的API-KEY"
        - 选择默认业务空间（或创建新空间）
        - 点击"确定"生成API密钥
        - 点击"查看"并复制生成的API密钥

    4. **获取API端点**  
        通义千问API地址：`https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation`

3. KIMI（月之暗面）
    
    KIMI是月之暗面公司推出的大语言模型，以长文本处理能力见长。官方文档：https://platform.moonshot.cn/docs/overview
    
    1. **注册KIMI账号**  
        访问KIMI控制台：https://platform.moonshot.cn/console
    
    2. **开通API权限**  
        - 登录后进入"产品中心"→"KIMI API"
        - 点击"立即开通"，完成基础配置
    
    3. **创建API密钥**  
        - 进入"控制台"→"API密钥"
        - 点击"创建密钥"，选择对应的项目空间
        - 复制生成的API Key和Secret（部分接口可能需要）
    
    4. **获取API端点**  
        KIMI对话API地址：`https://api.moonshot.cn/v1/chat/completions`

4. 火山方舟（字节跳动）

    火山方舟提供火山方舟大模型服务平台，支持多种模型如豆包等调用。官方文档：https://www.volcengine.com/docs/82379/1399008

    1. **注册火山方舟账号**  
        访问火山方舟官网：https://exp.volcengine.com/ark

    2. **开通大模型服务**  
        - 登录后进入"控制台"→"火山方舟"
        - 选择需要的模型（如"字节跳动-豆包"），点击"开通服务"
    
    3. **创建API密钥**  
        - 进入"访问控制"→"密钥管理"
        - 点击"新建密钥"，生成Access Key ID和Secret Access Key
        - 记录密钥信息（需妥善保管）
    
    4. **获取API端点**  
        火山方舟通用API地址：`https://ark.cn-beijing.volces.com/api/v3/chat/completions`

#### 1.1.1.2 环境变量设置

把API Key配置到环境变量，从而避免在代码里显式地配置API Key，降低泄漏风险：

1. 在Windows系统桌面中按Win+Q键，在搜索框中搜索编辑系统环境变量，单击打开系统属性界面。

2. 在系统属性窗口，单击环境变量，进入环境变量配置页面。

    ![p894015.png](attachment:p894015.png)

3. 在系统变量区域分别新建以下环境变量：
    - 深度求索：变量名`DEEPSEEK_API_KEY`，变量值填入Deepseek API Key
    - 通义千问：变量名`DASHSCOPE_API_KEY`，变量值填入DashScope API Key
    - KIMI：变量名`KIMI_API_KEY`，变量值填入KIMI API Key
    - 火山方舟：变量名`VOLCENGINE_ACCESS_KEY`和`VOLCENGINE_SECRET_KEY`，分别填入对应的密钥

4. 依次单击三个窗口的确定，关闭系统属性配置页面，完成环境变量配置。

#### 1.1.1.3 验证API配置

1. 使用命令提示符（CMD），运行以下命令验证各环境变量：
    ```bash
    # 验证深度求索
    echo %DEEPSEEK_API_KEY%
    # 验证通义千问
    echo %DASHSCOPE_API_KEY%
    # 验证KIMI
    echo %KIMI_API_KEY%
    # 验证火山方舟
    echo %VOLCENGINE_ACCESS_KEY%
    echo %VOLCENGINE_SECRET_KEY%
    ```
    如果正确显示对应API Key，说明环境变量配置成功。

2. 运行以下Python代码，验证各API配置是否成功：
    ```python
    import requests
    import os
    from dotenv import load_dotenv

    load_dotenv()

    def verify_deepseek_api():
        """验证深度求索API配置是否成功"""
        url = "https://api.deepseek.com/v1/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {os.getenv('DEEPSEEK_API_KEY')}"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": "你好，请返回'API配置成功'"}]
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            return "API配置成功" in response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"深度求索验证失败: {str(e)}")
            return False

    def verify_kimi_api():
        """验证KIMI API配置是否成功"""
        url = "https://api.moonshot.cn/v1/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {os.getenv('KIMI_API_KEY')}"
        }
        
        payload = {
            "model": "moonshot-v1-8k",
            "messages": [{"role": "user", "content": "你好，请返回'API配置成功'"}]
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            return "API配置成功" in response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"KIMI验证失败: {str(e)}")
            return False

    def verify_volcengine_api():
        """验证火山方舟API配置是否成功"""
        url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "X-Volc-AccessKey": os.getenv('VOLCENGINE_ACCESS_KEY'),
            "X-Volc-SecretKey": os.getenv('VOLCENGINE_SECRET_KEY')
        }
        
        payload = {
            "model": "doubao-pro",
            "messages": [{"role": "user", "content": "你好，请返回'API配置成功'"}]
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            return "API配置成功" in response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"火山方舟验证失败: {str(e)}")
            return False

    def verify_tongyi_api():
        """验证通义千问API配置是否成功"""
        url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {os.getenv('TONGYI_API_KEY')}"
        }
        
        payload = {
            "model": "qwen-plus",
            "input": {"prompt": "你好，请返回'API配置成功'"},
            "parameters": {"max_tokens": 50}
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            return "API配置成功" in response.json()["output"]["text"]
        except Exception as e:
            print(f"验证失败: {str(e)}")
            return False

    if __name__ == "__main__":
        if verify_deepseek_api():
            print("深度求索API配置成功")
        else:
            print("深度求索API配置失败")
            
        if verify_kimi_api():
            print("KIMI API配置成功")
        else:
            print("KIMI API配置失败")
            
        if verify_volcengine_api():
            print("火山方舟API配置成功")
        else:
            print("火山方舟API配置失败")
            
        if verify_tongyi_api():
            print("通义千问API配置成功")
        else:
            print("通义千问API配置失败")
    ```

#### 1.1.1.4 LLM API计费规则

在线LLM API的计费核心是**Token计量**，但不同服务商的Token计算逻辑、计费模式存在显著差异。以下结合主流服务商（深度求索、KIMI、火山引擎、通义千问）的规则展开说明：

1. Token概念
    - **定义**：Token是模型处理文本的基本单位，可理解为“语义片段”。例如：
        - 英文中，1个Token约等于4个字符（如“hello”为1个Token）；
        - 中文中，1个Token通常对应1个汉字（但部分服务会按词语拆分，如“人工智能”可能计为1个Token）。
    - **影响范围**：Token数量直接决定费用（按Token数计费）和模型处理能力（受上下文窗口限制，如“8k模型”指最大支持8192个Token的输入+输出）。

2. 主流服务商的Token计算与计费差异

    | 服务商       | Token计算规则                                                                 | 计费模式                                  | 特殊政策                                                                 |
    |--------------|-----------------------------------------------------------------------------|-----------------------------------------|----------------------------------------------------------------------|
    | 深度求索Deepseek [计费页](https://api-docs.deepseek.com/zh-cn/quick_start/pricing) | 中文按字符计（1汉字=1 Token），英文按分词计（1单词≈1 Token）；系统提示词和历史对话均计入Token。 | 输入/输出Token分别计费，不同模型单价不同（如`deepseek-chat`输入0.3元/千Token，输出0.6元/千Token）。 | 新用户赠送一定额度免费Token（约50万-100万），过期未使用自动清零。                       |
    | KIMI（月之暗面）[计费页](https://platform.moonshot.cn/docs/pricing) | 中英文统一按“字符数÷2”估算Token（如1000字符≈500 Token）；长文本处理时支持按实际分词结果精确计量。 | 输入/输出合并计费，`moonshot-v1-8k`模型统一0.3元/千Token，`moonshot-v1-32k`长文本模型单价略高。 | 免费额度每日限量（约10万Token/天），超出部分按阶梯价计费（用量越大单价越低）。            |
    | 火山方舟/豆包（字节跳动） [计费页](https://www.volcengine.com/docs/82379/1544681)| 中文1汉字=1 Token，英文1单词≈1 Token；上下文窗口内的所有内容（含系统提示、历史消息）均计入。 | 分模型计费，`doubao-pro`输入0.2元/千Token，输出0.4元/千Token；`doubao-lite`单价约为前者的1/3。 | 企业用户可购买套餐包（如100万Token套餐约150元），有效期1年，比按次计费便宜30%。         |
    | 通义千问 [计费页](https://help.aliyun.com/zh/model-studio/billing-for-model-studio?spm=5176.29597918.J_etFdgJIDidF3OAYTCTJFS.2.518c7b08zG1O5P)      | 中文1词语=1 Token，英文按BPE分词（1000字符≈300-400 Token）；输入/输出分别计量。         | 按模型档位收费，`qwen-plus`输入0.15元/千Token，输出0.3元/千Token；`qwen-max`单价约为其3倍。 | 阿里云账号实名认证后赠送100万免费Token，有效期3个月，仅支持基础模型。                   |


3. 计费常见问题
    1. **Token数量查询**  
        - 多数服务商提供API响应头（如`X-Token-Usage`）返回输入/输出Token数；  
        - 可通过官方工具预估（如KIMI的[Token计算器](https://platform.moonshot.cn/tools/token-calculator)）。

    2. **上下文窗口与超额计费**  
        - 若输入+输出Token超过模型最大窗口（如8k），会被截断或拒绝服务，需提前控制对话长度；  
        - 部分服务（如火山引擎）支持动态调整窗口，超额部分按溢价计费（约为基础价的1.5倍）。

    3. **免费额度限制**  
        - 免费Token通常仅支持基础模型，且有调用频率限制（如Deepseek每秒最多5次调用）；  
        - 商用场景需提前升级账号，避免因额度耗尽导致服务中断。

> 注：以上计费信息为2025年11月整理，具体以各服务商官方最新文档为准（如[火山引擎定价页](https://www.volcengine.com/product/ark/pricing)、[KIMI计费说明](https://platform.moonshot.cn/docs/pricing)）。

#### 1.1.1.5 四家服务商在线API：定价、模型特色及KIMI开源模型优势

以下是深度求索（Deepseek）、KIMI（月之暗面）、火山引擎（豆包）、通义千问（阿里）的**在线API核心信息整理**，聚焦定价体系、模型差异化特色，并重点解析KIMI新开源模型“逼近闭源”的核心优势（数据来源于2025年11月各服务商官网、权威评测榜单及技术白皮书）：

1. 深度求索（Deepseek）：代码与专业领域“专精型”选手
    - 在线API模型
        | 模型名称               | 上下文窗口 | 核心定位                  | 适用场景                          |
        |------------------------|------------|---------------------------|-----------------------------------|
        | deepseek-chat（基础）  | 8k/16k     | 通用对话、日常交互        | 客服、问答机器人、轻量交互        |
        | deepseek-coder-v2（代码）| 32k/64k   | 代码生成/调试/重构        | 程序员开发、自动化脚本、代码审核  |
        | deepseek-moe-7B/13B（进阶）| 64k     | 专业领域推理（数学/科研） | 学术论文撰写、数据分析、公式推导  |
        | deepseek-vl（多模态）  | 16k        | 图文理解+对话             | 图片解析、图文问答、视觉推理      |
    - 定价（2025年11月）
        | 模型类型       | 输入单价（元/千Token） | 输出单价（元/千Token） | 套餐包优惠（100万Token） | 免费额度          |
        |----------------|------------------------|------------------------|--------------------------|-------------------|
        | 通用对话（8k）  | 0.25                   | 0.50                   | 120元（省30%）           | 新用户50万Token（30天） |
        | 代码模型（32k） | 0.40                   | 0.80                   | 200元（省25%）           | 新用户30万Token（30天） |
        | MOE进阶模型    | 0.80                   | 1.60                   | 400元（省20%）           | 无免费额度        |
        | 多模态模型    | 0.60（文本）+ 0.1元/张图 | 1.20                   | 300元（省20%）           | 新用户10万Token+100张图 |
    - 模型特色
        - **代码领域天花板**：deepseek-coder-v2支持20+编程语言，代码生成准确率在HumanEval评测中达82.3%，超过GPT-4o-mini（79.8%），支持64k长代码上下文（如大型项目重构）。
        - **专业场景优化**：针对数学、物理、科研领域优化，MOE模型在MMLU（综合能力评测）中得分83.5，擅长复杂公式推导和学术写作。
        - **低延迟优势**：通用对话API响应时间≤300ms，适合实时交互场景（如直播弹幕回复、在线客服）。

2. KIMI（月之暗面）：长文本+开源模型“双王牌”，开源逼近闭源
    - 在线API模型
        | 模型名称               | 上下文窗口 | 核心定位                  | 适用场景                          |
        |------------------------|------------|---------------------------|-----------------------------------|
        | moonshot-v2-pro（闭源） | 256k       | 长文本+高精度对话         | 电子书解析、法律文书处理、万字报告生成 |
        | moonshot-v2-max（闭源） | 512k       | 超长篇文本+复杂推理       | 百万字小说分析、企业年报解读、多文档对比 |
        | moonshot-v2-open-70B（开源）| 128k    | 开源高精度模型            | 企业私有化部署（在线API支持调用）、成本敏感场景 |
        | moonshot-mini-1.8B（开源）| 32k      | 轻量开源模型              | 边缘设备、高并发低预算场景        |

    - 定价（2025年11月）
        | 模型类型       | 输入单价（元/千Token） | 输出单价（元/千Token） | 套餐包优惠（100万Token） | 免费额度          |
        |----------------|------------------------|------------------------|--------------------------|-------------------|
        | v2-pro（256k）  | 0.35                   | 0.70                   | 175元（省30%）           | 每日10万Token（不限期） |
        | v2-max（512k）  | 0.70                   | 1.40                   | 350元（省30%）           | 无免费额度        |
        | v2-open-70B（开源）| 0.20               | 0.40                   | 100元（省33%）           | 新用户80万Token（60天） |
        | mini-1.8B（开源） | 0.05                   | 0.10                   | 25元（省33%）            | 每日20万Token（不限期） |

    - 模型特色（重点：开源模型逼近闭源）
        - **长文本处理绝对优势**：512k上下文窗口支持“一次性解析百万字文本”，无需分段处理，在法律合同、学术论文、小说分析场景中效率远超同类模型（多数竞品最大窗口为128k）。
        - **开源模型性能突破**：
            - **moonshot-v2-open-70B**：在2025年10月Hugging Face Open LLM Leaderboard中，MMLU得分85.7（闭源模型GPT-4o-mini为88.2，豆包pro为86.3），C-Eval（中文综合能力）得分91.2（超过GPT-4o-mini的89.5），推理、逻辑、中文理解能力已逼近主流闭源模型。
            - **核心优势**：支持128k长上下文，在线API调用成本仅为闭源模型的57%（v2-open-70B输入0.2元/千Token vs v2-pro 0.35元/千Token），且开源可商用（协议宽松，支持企业二次开发）。
        - **多轮对话一致性强**：针对长对话优化，连续50+轮交互不丢失上下文，适合智能助手、虚拟客服等场景。

3. 火山引擎（豆包）：多模态+企业级“生态型”模型
    - 在线API模型
        | 模型名称               | 上下文窗口 | 核心定位                  | 适用场景                          |
        |------------------------|------------|---------------------------|-----------------------------------|
        | doubao-lite（基础）    | 8k         | 轻量通用对话              | 小程序、APP内嵌交互、低预算场景    |
        | doubao-pro-2（进阶）   | 64k        | 通用+多模态（图文/语音）  | 企业客服、内容生成、语音转文字交互 |
        | doubao-max-2（旗舰）   | 128k       | 复杂推理+工具调用         | 企业级数据分析、智能办公、API联动 |
        | doubao-vision（多模态）| 32k        | 图文生成+理解             | 海报设计、图片编辑、视觉问答      |

    - 定价（2025年11月）
        | 模型类型       | 输入单价（元/千Token） | 输出单价（元/千Token） | 套餐包优惠（100万Token） | 免费额度          |
        |----------------|------------------------|------------------------|--------------------------|-------------------|
        | doubao-lite    | 0.10                   | 0.20                   | 50元（省33%）            | 每日30万Token（不限期） |
        | doubao-pro-2   | 0.25                   | 0.50                   | 125元（省30%）           | 新用户60万Token（45天） |
        | doubao-max-2   | 0.60                   | 1.20                   | 300元（省25%）           | 新用户20万Token（30天） |
        | 多模态（图文）  | 0.30（文本）+ 0.15元/张图 | 0.60                   | 180元（省25%）           | 新用户10万Token+200张图 |

    - 模型特色
        - **多模态能力全面**：支持图文理解、语音交互、图片生成一体化，doubao-vision生成图片的分辨率可达4K，且支持“文本描述+参考图”混合生成，适合内容创作场景。
        - **企业级服务优势**：提供私有化部署、数据隔离、定制训练服务，适配金融、政务等敏感行业，支持SLA保障（可用性99.99%）。
        - **生态整合强**：无缝对接火山引擎的云服务器、存储、CDN等产品，适合需要全链路技术支持的企业用户。

4. 通义千问（阿里）：阿里云生态+高性能“通用型”模型
    - 核心在线API模型矩阵
        | 模型名称               | 上下文窗口 | 核心定位                  | 适用场景                          |
        |------------------------|------------|---------------------------|-----------------------------------|
        | qwen-lite（基础）      | 8k         | 轻量通用对话              | 高并发场景、低成本交互            |
        | qwen-plus（进阶）      | 64k        | 通用高性能对话            | 内容生成、智能客服、知识问答      |
        | qwen-max-turbo（旗舰） | 128k       | 复杂推理+工具调用         | 企业级决策支持、数据分析、代码生成 |
        | qwen-vl-plus（多模态） | 32k        | 图文理解+生成             | 电商商品图生成、文档OCR、视觉推理 |

    - 定价（2025年11月）
        | 模型类型       | 输入单价（元/千Token） | 输出单价（元/千Token） | 套餐包优惠（100万Token） | 免费额度          |
        |----------------|------------------------|------------------------|--------------------------|-------------------|
        | qwen-lite      | 0.08                   | 0.16                   | 40元（省33%）            | 每日20万Token（不限期） |
        | qwen-plus      | 0.20                   | 0.40                   | 100元（省33%）           | 实名认证后100万Token（90天） |
        | qwen-max-turbo | 0.80                   | 1.60                   | 400元（省20%）           | 新用户30万Token（30天） |
        | 多模态（图文）  | 0.25（文本）+ 0.12元/张图 | 0.50                   | 150元（省30%）           | 新用户10万Token+150张图 |

    - 模型特色
        - **阿里云生态联动**：无缝对接阿里云OSS、RDS、函数计算等产品，适合在阿里云体系内开发的企业，可降低跨平台集成成本。
        - **通用性能强劲**：qwen-max-turbo在MMLU得分89.1，接近GPT-4o（92.3），支持复杂逻辑推理、多文档汇总、代码生成等全场景任务。
        - **中文优化极致**：针对中文语义、文化语境深度优化，在方言理解、古文解析、中文内容生成场景中准确率高于多数竞品。


四家服务商在线API横向对比

| 维度                | 深度求索（Deepseek）       | KIMI（月之暗面）           | 火山引擎（豆包）           | 通义千问（阿里）           |
|---------------------|----------------------------|----------------------------|----------------------------|----------------------------|
| 核心优势            | 代码生成、专业领域推理     | 长文本处理、开源模型逼近闭源 | 多模态、企业级服务         | 阿里云生态、中文通用性能   |
| 最大上下文窗口      | 64k（代码模型）            | 512k（v2-max）             | 128k（doubao-max-2）       | 128k（qwen-max-turbo）     |
| 开源模型支持（在线API） | 无                        | 有（v2-open-70B/mini-1.8B） | 无                        | 无                        |
| 多模态能力          | 图文理解                   | 文本为主（支持长文本OCR）   | 图文/语音/生成一体化       | 图文理解+生成             |
| 性价比（千Token成本）| 中高（代码模型偏贵）       | 高（开源模型成本低）       | 中（企业套餐优惠大）       | 中低（基础模型便宜）       |
| 免费额度            | 新用户50万Token（30天）    | 每日10万Token（不限期）    | 每日30万Token（lite模型）  | 实名认证100万Token（90天） |
| 适用场景            | 程序员、科研人员           | 长文本处理、成本敏感用户   | 企业级多模态、敏感行业     | 阿里云生态用户、通用场景   |

### 1.1.2 本地LLM部署（Qwen 4B模型）

#### 部署步骤
1. **安装Ollama工具**：访问https://ollama.com/download
2. **下载Qwen 4B模型**：`ollama pull qwen:4b`
3. **验证部署**：`ollama run qwen:4b`，输入"你好"测试响应

## 1.2. 使用Python脚本调用LLM

### 1.2.1 调用在线API（通义千问）

#### 调用代码
```python
import requests
import os
from dotenv import load_dotenv

load_dotenv()

def call_tongyi_api(prompt):
    """调用通义千问API"""
    response = requests.post(
        "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {os.getenv('TONGYI_API_KEY')}"
        },
        json={
            "model": "qwen-plus",
            "input": {"prompt": prompt},
            "parameters": {"temperature": 0.7}
        }
    )
    
    return response.json()["output"]["text"]

# 使用示例
if __name__ == "__main__":
    print(call_tongyi_api("分析中小企业使用CRM的收益"))
```

### 1.2.2 调用本地Qwen模型

#### 基础调用代码
```python
import requests

def call_local_llm(prompt, model="qwen:4b", temperature=0.7):
    """调用本地Qwen模型"""
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": temperature}
        }
    )
    
    return response.json()["response"]

# 使用示例
if __name__ == "__main__":
    prompt = "分析智能手表的3个核心客户群体"
    print(call_local_llm(prompt))
```

## 1.3 使用Cherry Studio调用LLM

### 1.3.1 Cherry Studio简介

#### 工具特点
- 可视化界面设计，无需编程
- 支持多种LLM模型集成
- 内置提示词管理和测试功能

### 1.3.2 下载与安装
1. 访问Cherry Studio官网：https://www.cherryai.com/
2. 下载并安装对应操作系统版本
3. 完成注册并登录

### 1.3.3 配置本地Qwen模型
1. 进入"模型管理"→"添加模型"
2. 选择"本地Ollama"，系统自动检测Qwen模型
3. 选择"qwen:4b"，点击"测试连接"验证

### 1.3.4 配置通义千问在线模型

#### 在Cherry Studio中配置
1. 打开Cherry Studio，进入"设置"→"模型服务"
2. 找到"阿里云百炼"，粘贴已有的API密钥
3. 点击"保存"并"测试连接"
4. 进入"模型管理"，添加通义千问模型（如qwen-plus）

#### 验证配置
1. 创建简单应用，添加"LLM调用"组件
2. 选择通义千问模型，输入提示词测试
3. 运行应用，确认能正常生成响应

### 1.3.5 本地与在线模型对比

| 特性 | 本地Qwen模型 | 通义千问在线模型 |
|------|-------------|----------------|
| 数据隐私 | 高（本地处理） | 低（数据上传） |
| 使用成本 | 一次性下载 | 按Token计费 |
| 网络依赖 | 无 | 必须联网 |
| 模型更新 | 手动更新 | 自动更新 |
| 性能要求 | 较高（需本地资源） | 较低（云端处理） |

## 1.4 系统提示词设计

### 1.4.1 基础结构
```markdown
# 系统角色
你是[专业角色]，拥有[经验背景]。

## 任务要求
[详细描述任务内容和目标]

## 输出格式
[指定回答的结构和格式]
```

### 1.4.2 脑筋急转弯生成示例
```markdown
# 系统角色
你是专业谜语创作者，擅长设计中文脑筋急转弯。

## 难度要求
- 简单：适合儿童，答案直接
- 困难：包含双关语或多步推理

## 输出格式
**题目**：[脑筋急转弯题目]
**答案**：[简洁回答]
**解析**：[解释幽默原理]
```

## 1.5 课程作业：定制脑筋急转弯LLM

### 1.5.1 作业目标
使用Cherry Studio开发脑筋急转弯生成器，支持在线/本地模型切换，能生成不同难度的题目并解释答案。

### 1.5.2 具体要求
1. **功能要求**：
   - 支持简单/困难难度选择
   - 可输入自定义主题
   - 显示题目、答案和解析

2. **实现步骤**：
   - 配置至少一种模型（本地Qwen或在线通义千问）
   - 设计系统提示词定义生成规则
   - 创建包含难度选择的交互界面
   - 测试并优化生成效果

3. **提交内容**：
   - 3个生成示例（含解析）
   - 应用界面截图
   - 开发总结（说明模型选择理由）